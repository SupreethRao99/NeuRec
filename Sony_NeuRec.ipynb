{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sony-NeuRec.ipynb",
      "provenance": [],
      "mount_file_id": "1n-_DQ8t8PXpN3XwfGQQBA9ReU4Egl5vl",
      "authorship_tag": "ABX9TyMqwExSPiIkSY6RWsRWlrWm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SupreethRao99/NeuRec/blob/main/Sony_NeuRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeuRec\n",
        "Recommending Content using Neural Colaborative Filtering. \n",
        "\n",
        "Trained Model checkpoint can be found [here](https://drive.google.com/drive/folders/1--3T3Mn0L0UCAH0thAkINIL2I-hdKNNA?usp=sharing)\n",
        "\n",
        "Model Achieves 72% training recall score and 99.9% validation score\n"
      ],
      "metadata": {
        "id": "_Z09QI7EaMza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "m8CjUq6umAeh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEEqyWB4l_3c"
      },
      "outputs": [],
      "source": [
        "# installing required libraries\n",
        "%%capture\n",
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "!pip install -q tensorflow-ranking\n",
        "!pip install -q tf-nightly\n",
        "!pip install -q ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import os\n",
        "import random as rn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_ranking as tfr\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow import keras\n",
        "import ml_collections"
      ],
      "metadata": {
        "id": "eMwbDJRKmIky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Dataset is stored on google drive for easy acess on Google Colab\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sf7elwHmStY",
        "outputId": "36e538cb-b725-459b-c98e-1b1972048057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Hyperparameter dictionary for easy experimentation and hyperparameter\n",
        "# optmization \n",
        "\n",
        "def model_config():\n",
        "  cfg_dictionary = {\n",
        "      \"root_dir\":'/content/drive/MyDrive/Sony Dataset',\n",
        "      \"relationships_file\":\"relationship.csv\",\n",
        "      \"content_file\":\"content.csv\",\n",
        "\n",
        "      \"validation_split\": 0.9,\n",
        "\n",
        "      \"epochs\": 10,\n",
        "      \"batch_size\": 256,\n",
        "\n",
        "      \"embedding_size\": 256,\n",
        "      \"random_seed\": 42,\n",
        "      \"model_checkpoint\": \"NCF99\",\n",
        "  }\n",
        "  cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
        "\n",
        "  return cfg\n",
        "\n",
        "cfg = model_config()"
      ],
      "metadata": {
        "id": "ODBBC-2u5eQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting random seed for experiment reproducibility \n",
        "def set_seed(seed=cfg.random_seed):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    rn.seed(seed)\n",
        "set_seed(cfg.random_seed)"
      ],
      "metadata": {
        "id": "l2TsggstmQDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre Processing"
      ],
      "metadata": {
        "id": "qZgsH0dYmirS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(root_dir, relationships_file, content_file):\n",
        "  required_columns = [\"user_id\", \"content_id\", \"rating\", \"date\"]\n",
        "\n",
        "  df1 = pd.read_csv(os.path.join(root_dir, relationships_file))\n",
        "  df2 = pd.read_csv(os.path.join(root_dir, content_file))\n",
        "  joined_df = pd.merge(df1, df2, on=\"content_id\", how=\"left\")\n",
        "  df = joined_df[required_columns]\n",
        "  df = df.sort_values(\"date\")\n",
        "  df = df.astype({\"rating\": float})\n",
        "  return df"
      ],
      "metadata": {
        "id": "chK4AJ4bmhkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_df(cfg.root_dir, cfg.relationships_file, cfg.content_file)\n",
        "user_ids = df[\"user_id\"].unique().tolist()\n",
        "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
        "\n",
        "content_ids = df[\"content_id\"].unique().tolist()\n",
        "content2content_encoded = {x: i for i, x in enumerate(content_ids)}\n",
        "content_encoded2content = {i: x for i, x in enumerate(content_ids)}\n",
        "df[\"user\"] = df[\"user_id\"].map(user2user_encoded)\n",
        "df[\"content\"] = df[\"content_id\"].map(content2content_encoded)\n",
        "\n",
        "num_users = len(user2user_encoded)\n",
        "num_content = len(content_encoded2content)\n",
        "df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n",
        "# min and max ratings will be used to normalize the ratings later\n",
        "min_rating = min(df[\"rating\"])\n",
        "max_rating = max(df[\"rating\"])\n",
        "\n",
        "print(\n",
        "    f\"Number of users: {num_users}, Number of Movies: {num_content}, Min rating: {min_rating}, Max rating: {max_rating}\"\n",
        ")\n",
        "\n",
        "df = df.sample(frac=1, random_state=1490251)\n",
        "x = df[[\"user\", \"content\"]].values\n",
        "y = (\n",
        "    df[\"rating\"]\n",
        "    .apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n",
        "    .values\n",
        ")\n",
        "train_indices = int(cfg.validation_split * df.shape[0])\n",
        "x_train, x_val, y_train, y_val = (\n",
        "    x[:train_indices],\n",
        "    x[train_indices:],\n",
        "    y[:train_indices],\n",
        "    y[train_indices:],\n",
        ")"
      ],
      "metadata": {
        "id": "CylISnLy1Wwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25af4885-855a-48c4-dff1-457a7f1ea71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users: 10923, Number of Movies: 44223, Min rating: 0.0, Max rating: 10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "hRdhgww1mwgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommenderNet(keras.Model):\n",
        "    def __init__(self, num_users, num_content, embedding_size, **kwargs):\n",
        "      super(RecommenderNet, self).__init__(**kwargs)\n",
        "      self.num_users = num_users\n",
        "      self.num_content = num_content\n",
        "      self.embedding_size = embedding_size\n",
        "      self.user_embedding = layers.Embedding(\n",
        "          num_users,\n",
        "          embedding_size,\n",
        "          embeddings_initializer=\"he_normal\",\n",
        "          embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "      )\n",
        "      self.user_bias = layers.Embedding(num_users, 1)\n",
        "      self.content_embedding = layers.Embedding(\n",
        "          num_content,\n",
        "          embedding_size,\n",
        "          embeddings_initializer=\"he_normal\",\n",
        "          embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
        "      )\n",
        "      self.content_bias = layers.Embedding(num_content, 1)\n",
        "\n",
        "      # Add dense layers head to the model.\n",
        "      self.d1 = Dense(1024, activation=\"relu\")\n",
        "      self.d2 = Dense(512, activation=\"relu\")\n",
        "      self.d3 = Dense(64, activation=\"relu\")\n",
        "      self.d4 = Dense(1)\n",
        "\n",
        "      self.dr1 = Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      user_vector = self.user_embedding(inputs[:, 0])\n",
        "      user_bias = self.user_bias(inputs[:, 0])\n",
        "      content_vector = self.content_embedding(inputs[:, 1])\n",
        "      content_bias = self.content_bias(inputs[:, 1])\n",
        "      dot_user_content = tf.tensordot(user_vector, content_vector, 2)\n",
        "      # Add all the components (including bias)\n",
        "      x = dot_user_content + user_bias + content_bias\n",
        "      x = self.d1(x)\n",
        "      x = self.dr1(x)\n",
        "      x = self.d2(x)\n",
        "      x = self.dr1(x)\n",
        "      x = self.d3(x)\n",
        "      x = self.dr1(x)\n",
        "      x = self.d4(x)\n",
        "\n",
        "      # The sigmoid activation forces the rating to between 0 and 1\n",
        "      return tf.nn.sigmoid(x)\n"
      ],
      "metadata": {
        "id": "TgK00U5Wn0IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "hm0BaFfCmyda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RecommenderNet(num_users, num_content , cfg.embedding_size)\n",
        "\n",
        "model.compile(\n",
        "    loss=tfr.keras.losses.PairwiseHingeLoss(),\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[tf.keras.metrics.Recall()],\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=3),\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", patience=1),\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=cfg.batch_size,\n",
        "    epochs=cfg.epochs,\n",
        "    verbose=1,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.save(cfg.model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A_OP5FVoEDN",
        "outputId": "98d3e20d-26d3-4e9d-d57b-3ec9a1e0d391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5817/5817 [==============================] - 110s 18ms/step - loss: 1.8290e-05 - recall: 0.7206 - val_loss: 0.0000e+00 - val_recall: 0.9999 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "5817/5817 [==============================] - 103s 18ms/step - loss: 0.0000e+00 - recall: 0.7214 - val_loss: 0.0000e+00 - val_recall: 0.9999 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "5817/5817 [==============================] - 103s 18ms/step - loss: 0.0000e+00 - recall: 0.7209 - val_loss: 0.0000e+00 - val_recall: 0.9999 - lr: 1.0000e-04\n",
            "Epoch 4/10\n",
            "5817/5817 [==============================] - 102s 18ms/step - loss: 0.0000e+00 - recall: 0.7204 - val_loss: 0.0000e+00 - val_recall: 0.9999 - lr: 1.0000e-05\n",
            "INFO:tensorflow:Assets written to: NCF99/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "hO0_Mbd-nCnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading saved model\n",
        "model = tf.keras.models.load_model(\"/content/NCF99\")"
      ],
      "metadata": {
        "id": "UN4PykSPIk6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = pd.read_csv(os.path.join(cfg.root_dir,\"content.csv\"))\n",
        "recommendations = {}\n",
        "def make_recommendations(user_id):\n",
        "  recs = []\n",
        "  content_watched_by_user = df[df.user_id == user_id]\n",
        "  content_not_watched = content[\n",
        "    ~content[\"content_id\"].isin(content_watched_by_user.content_id.values)\n",
        "    ][\"content_id\"]\n",
        "  content_not_watched = list(\n",
        "    set(content_not_watched).intersection(set(content2content_encoded.keys()))\n",
        "  )\n",
        "  content_not_watched = [[content2content_encoded.get(x)] for x in content_not_watched]\n",
        "  user_encoder = user2user_encoded.get(user_id)\n",
        "  user_content_array = np.hstack(\n",
        "    ([[user_encoder]] * len(content_not_watched), content_not_watched)\n",
        "  )\n",
        "  ratings = model.predict(user_content_array).flatten()\n",
        "  top_ratings_indices = ratings.argsort()[-10:][::-1]\n",
        "  recommended_content_ids = [\n",
        "    content_encoded2content.get(content_not_watched[x][0]) for x in top_ratings_indices\n",
        "  ]\n",
        "  recommended_content = content[content[\"content_id\"].isin(recommended_content_ids)]\n",
        "  for row in recommended_content.itertuples():\n",
        "    recs.append(row.content_id)\n",
        "\n",
        "  recommendations[user_id]=recs"
      ],
      "metadata": {
        "id": "9UjC_0vp8R78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm as tqdm\n",
        "test_df = pd.read_csv(os.path.join(cfg.root_dir,\"test.csv\"))\n",
        "test_list = list(test_df[\"user_id\"])\n",
        "for user in tqdm.tqdm(test_list):\n",
        "  try:\n",
        "    make_recommendations(user)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "2fv9aeT0Cibw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Results "
      ],
      "metadata": {
        "id": "ol9DFm2ZnOYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('submission.json', 'w') as fp:\n",
        "    json.dump(recommendations, fp)"
      ],
      "metadata": {
        "id": "AA0lknBADJDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Done Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8oQI2f5M25y",
        "outputId": "48a4700e-6d68-45f7-d7f5-b8893c9bfae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done Done!\n"
          ]
        }
      ]
    }
  ]
}